{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae0fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory before loading model: 0.52 GB\n",
      "\n",
      "Memory after loading tokenizer: 0.52 GB\n",
      "Tokenizer memory usage: 0.01 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d159403ec2b4e45b4855ade019aa881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting and de-quantizing GGUF tensors...:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory after loading model: 1.00 GB\n",
      "Model memory usage: 0.48 GB\n",
      "\n",
      "110638080\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import youtokentome as yttm\n",
    "from transformers import LlamaForCausalLM\n",
    "import torch\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Measure memory before loading model\n",
    "process = psutil.Process(os.getpid())\n",
    "memory_before_model = process.memory_info().rss / 1024**3\n",
    "print(f\"\\nMemory before loading model: {memory_before_model:.2f} GB\")\n",
    "\n",
    "bpe = yttm.BPE(model=\"JetBrains_model/flcc.bpe\")\n",
    "\n",
    "# Measure memory after tokenizer\n",
    "memory_after_tokenizer = process.memory_info().rss / 1024**3\n",
    "print(f\"\\nMemory after loading tokenizer: {memory_after_tokenizer:.2f} GB\")\n",
    "print(f\"Tokenizer memory usage: {memory_after_tokenizer - memory_before_model:.2f} GB\")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"JetBrains_model\", gguf_file=\"flcc.model\", torch_dtype=torch.float32)\n",
    "\n",
    "# Measure memory after model loading\n",
    "memory_after_model = process.memory_info().rss / 1024**3\n",
    "print(f\"\\nMemory after loading model: {memory_after_model:.2f} GB\")\n",
    "print(f\"Model memory usage: {memory_after_model - memory_after_tokenizer:.2f} GB\\n\")\n",
    "\n",
    "print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac7d3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- System Memory Usage After Model Loading ---\n",
      "Total Process Memory Usage: 1.00 GB (1029.0 MB)\n",
      "Estimated Model + Tokenizer Memory: 0.49 GB\n",
      "\n",
      "Token IDs: [4, 1004, 965, 90, 10555, 4, 9261, 6027, 4, 3150, 6027, 4, 2611, 4, 2274, 4, 3789, 4, 1462, 4, 2641, 4, 572, 53, 8378, 1719, 1010, 649, 3374, 4, 665, 7013, 2456, 1010, 412, 4, 665, 15992, 6956, 1654, 348, 1100, 412, 4, 665, 7013, 2456, 5110, 1301, 412, 4, 665, 1937, 1203, 5110, 1301, 412, 4, 665, 7013, 2456, 170, 5110, 412, 4, 665, 7013, 2456, 4264, 1301, 412, 4, 665, 1937, 1203, 4264, 1301, 412, 4, 665, 7013, 2456, 853, 412, 4, 665, 7013, 2456, 5110, 853, 412, 4, 665, 7013, 2456, 4264, 853, 412, 4, 665, 10962, 8378, 1719, 1010, 649, 1264]\n",
      "Decoded back: \n",
      "package com.codelm;\n",
      "import java.io.BufferedWriter;\n",
      "import java.io.FileWriter;\n",
      "import java.io.IOException;\n",
      "import java.util.ArrayList;\n",
      "import java.util.HashMap;\n",
      "import java.util.List;\n",
      "import java.util.Map;\n",
      "public class CumulativeTokenStats {\n",
      "    private long totalTokens;\n",
      "    private Map<String, Long> tokenTypeCounts;\n",
      "    private long totalLiteralChars;\n",
      "    private int maxLiteralChars;\n",
      "    private long totalStringLiterals;\n",
      "    private long totalIdentifierChars;\n",
      "    private int maxIdentifierChars;\n",
      "    private long totalBytes;\n",
      "    private long totalLiteralBytes;\n",
      "    private long totalIdentifierBytes;\n",
      "    public CumulativeTokenStats() {\n",
      "\n",
      "Memory before generation: 1.01 GB\n",
      "\n",
      "Memory after generation: 1.08 GB\n",
      "Generation memory overhead: 0.08 GB\n",
      "Total memory increase from start: 0.57 GB\n",
      "Raw output tokens: [4, 1004, 965, 90, 10555, 4, 9261, 6027, 4, 3150, 6027, 4, 2611, 4, 2274, 4, 3789, 4, 1462, 4, 2641, 4, 572, 53, 8378, 1719, 1010, 649, 3374, 4, 665, 7013, 2456, 1010, 412, 4, 665, 15992, 6956, 1654, 348, 1100, 412, 4, 665, 7013, 2456, 5110, 1301, 412, 4, 665, 1937, 1203, 5110, 1301, 412, 4, 665, 7013, 2456, 170, 5110, 412, 4, 665, 7013, 2456, 4264, 1301, 412, 4, 665, 1937, 1203, 4264, 1301, 412, 4, 665, 7013, 2456, 853, 412, 4, 665, 7013, 2456, 5110, 853, 412, 4, 665, 7013, 2456, 4264, 853, 412, 4, 665, 10962, 8378, 1719, 1010, 649, 1264, 4, 5154, 4, 36, 4, 36, 60, 4, 1084, 2456, 60, 4, 36, 4, 363, 4, 163, 708, 4, 7, 4, 36, 4, 363, 4, 163, 708, 4, 7, 4, 36, 4, 36, 4, 363, 4, 163, 708, 4, 7, 4, 36, 4, 36, 4, 363, 4, 163, 708, 4]\n",
      "Input prompt:\n",
      "package com.codelm;\n",
      "\n",
      "import java.io.BufferedWriter;\n",
      "import java.io.FileWriter;\n",
      "import java.io.IOException;\n",
      "import java.util.ArrayList;\n",
      "import java.util.HashMap;\n",
      "import java.util.List;\n",
      "import java.util.Map;\n",
      "\n",
      "public class CumulativeTokenStats {\n",
      "\n",
      "    private long totalTokens;\n",
      "    private Map<String, Long> tokenTypeCounts;\n",
      "    private long totalLiteralChars;\n",
      "    private int maxLiteralChars;\n",
      "    private long totalStringLiterals;\n",
      "    private long totalIdentifierChars;\n",
      "    private int maxIdentifierChars;\n",
      "    private long totalBytes;\n",
      "    private long totalLiteralBytes;\n",
      "    private long totalIdentifierBytes;\n",
      "\n",
      "    public CumulativeTokenStats() {\n",
      "\n",
      "Generated completion:\n",
      "\n",
      "return 0;\n",
      "}\n",
      "};\n",
      "long total;\n",
      "}\n",
      "@Override\n",
      "public long\n",
      "{\n",
      "}\n",
      "@Override\n",
      "public long\n",
      "{\n",
      "}\n",
      "}\n",
      "@Override\n",
      "public long\n",
      "{\n",
      "}\n",
      "}\n",
      "@Override\n",
      "public long\n",
      "\n",
      "Generated 50 tokens in 2.173s → 23.0 tokens/s\n"
     ]
    }
   ],
   "source": [
    "prefix_code = \"\"\"\n",
    "public class HelloWorld {\n",
    "    public static void main(String[] args) {\n",
    "        System.out.\n",
    "\"\"\"\n",
    "\n",
    "prefix_code2 = \"\"\"package com.codelm;\n",
    "\n",
    "import java.io.BufferedWriter;\n",
    "import java.io.FileWriter;\n",
    "import java.io.IOException;\n",
    "import java.util.ArrayList;\n",
    "import java.util.HashMap;\n",
    "import java.util.List;\n",
    "import java.util.Map;\n",
    "\n",
    "public class CumulativeTokenStats {\n",
    "\n",
    "    private long totalTokens;\n",
    "    private Map<String, Long> tokenTypeCounts;\n",
    "    private long totalLiteralChars;\n",
    "    private int maxLiteralChars;\n",
    "    private long totalStringLiterals;\n",
    "    private long totalIdentifierChars;\n",
    "    private int maxIdentifierChars;\n",
    "    private long totalBytes;\n",
    "    private long totalLiteralBytes;\n",
    "    private long totalIdentifierBytes;\n",
    "\n",
    "    public CumulativeTokenStats() {\"\"\"\n",
    "\n",
    "# System memory usage\n",
    "system_memory_mb = memory_after_model * 1024  # Convert back to MB for display\n",
    "print(f\"\\n--- System Memory Usage After Model Loading ---\")\n",
    "print(f\"Total Process Memory Usage: {memory_after_model:.2f} GB ({system_memory_mb:.1f} MB)\")\n",
    "print(f\"Estimated Model + Tokenizer Memory: {memory_after_model - memory_before_model:.2f} GB\\n\")\n",
    "\n",
    "tokens = bpe.encode(prefix_code2, output_type=yttm.OutputType.ID)\n",
    "\n",
    "# Print tokenization to verify\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Decoded back: {bpe.decode([tokens])[0]}\")\n",
    "\n",
    "# Convert to tensor\n",
    "input_ids = torch.tensor([tokens]).to(model.device)\n",
    "attention_mask = torch.ones_like(input_ids).to(model.device)\n",
    "input_len = input_ids.shape[-1]\n",
    "\n",
    "# Monitor memory before generation\n",
    "memory_before_generation = process.memory_info().rss / 1024**3\n",
    "print(f\"\\nMemory before generation: {memory_before_generation:.2f} GB\")\n",
    "\n",
    "# Generate completions\n",
    "start = time.perf_counter()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=50,  # Controls how many new tokens to generate\n",
    "        num_beams=4,\n",
    "        num_return_sequences=4,\n",
    "        pad_token_id=model.config.eos_token_id,  # Set padding token to EOS token\n",
    "        #repetition_penalty=1.2\n",
    "    )\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "# Final system memory check\n",
    "final_system_memory_gb = process.memory_info().rss / 1024**3\n",
    "print(f\"\\nMemory after generation: {final_system_memory_gb:.2f} GB\")\n",
    "print(f\"Generation memory overhead: {final_system_memory_gb - memory_before_generation:.2f} GB\")\n",
    "print(f\"Total memory increase from start: {final_system_memory_gb - memory_before_model:.2f} GB\")\n",
    "\n",
    "# Count how many new tokens were generated and tokens/sec\n",
    "generated_ids = outputs[0]\n",
    "total_len = generated_ids.shape[-1]\n",
    "new_tokens = total_len - input_len\n",
    "duration = end - start\n",
    "toks_per_sec = new_tokens / duration\n",
    "\n",
    "# Print the raw output to understand what's happening\n",
    "print(\"Raw output tokens:\", outputs[0].tolist())\n",
    "\n",
    "# Get the generated content after the prompt\n",
    "original_length = len(tokens)\n",
    "generated_tokens = outputs[0][original_length:].tolist()\n",
    "\n",
    "# Decode only the newly generated tokens\n",
    "completion = bpe.decode([generated_tokens])[0]\n",
    "\n",
    "# Format the output\n",
    "print(\"Input prompt:\")\n",
    "print(prefix_code2.strip())\n",
    "print(\"\\nGenerated completion:\")\n",
    "print(f\"{completion}\")\n",
    "\n",
    "print(f\"Generated {new_tokens} tokens in {duration:.3f}s → {toks_per_sec:.1f} tokens/s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
